{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SM4Tops Workflow Welcome to the SM4Tops workflow documentation. This project provides a reproducible workflow to start a MVA (Multivariate Analysis) training for four-top Monte Carlo (MC) samples using Snakemake . Introduction The goal of this workflow is to facilitate MVA training for four-top processes in the ATLAS experiment to improve the separation between signal and background. It includes: Preprocessing MC samples with FastFrames Producing analysis-ready ntuples (Root files) by submitting jobs to HTCondor Converting the ntuples into h5 files through dumper Preparation of data that fits the input format for the training later in SALT using umami Automating reproducible training steps for machine learning models This workflow is designed for researchers who want to: Reproduce MVA training inputs reliably Process large MC samples more efficiently Resume long-running workflows safely Workflow Overview The workflow contains the following steps: Ntuple production using FastFrames and HTCondor Synchronization with EOS to ensure ntuples are available Conversion of ntuples into HDF5 format for machine learning Preprocessing of inputs using Umami for downstream training","title":"Home"},{"location":"#sm4tops-workflow","text":"Welcome to the SM4Tops workflow documentation. This project provides a reproducible workflow to start a MVA (Multivariate Analysis) training for four-top Monte Carlo (MC) samples using Snakemake .","title":"SM4Tops Workflow"},{"location":"#introduction","text":"The goal of this workflow is to facilitate MVA training for four-top processes in the ATLAS experiment to improve the separation between signal and background. It includes: Preprocessing MC samples with FastFrames Producing analysis-ready ntuples (Root files) by submitting jobs to HTCondor Converting the ntuples into h5 files through dumper Preparation of data that fits the input format for the training later in SALT using umami Automating reproducible training steps for machine learning models This workflow is designed for researchers who want to: Reproduce MVA training inputs reliably Process large MC samples more efficiently Resume long-running workflows safely","title":"Introduction"},{"location":"#workflow-overview","text":"The workflow contains the following steps: Ntuple production using FastFrames and HTCondor Synchronization with EOS to ensure ntuples are available Conversion of ntuples into HDF5 format for machine learning Preprocessing of inputs using Umami for downstream training","title":"Workflow Overview"},{"location":"faq/","text":"FAQ Q1: Snakemake reports missing output files? This usually happens because the output directory is on EOS or AFS , which may have some latency in reflecting newly created files. Solutions: Add the --latency-wait option when running Snakemake. Example: snakemake -j1 --latency-wait 600 This tells Snakemake to wait up to 600 seconds for missing files to appear. Make sure all Condor jobs have finished before running Snakemake again. You can check with: condor_q Q2: How to safely cancel running Condor jobs? Identify the jobs with: condor_q Cancel a specific job or job cluster with: condor_rm <job_id> Q3: What if my logs don\u2019t appear? Ensure the log directory exists and is writable. If you run Condor jobs, logs may only appear after the job finishes on the batch system. Check the wrapper script path and permissions. Q4: Why does Snakemake skip the ntuple submission step? If the ntuple directory already exists in EOS: - Submission is skipped - A message is printed - he workflow continues safely This prevents accidental double submission to HTCondor. Q5: Snakemake says \u201cNothing to be done\u201d but I expected more steps to run This workflow uses flag files in the flags/ directory to track progress. If a flag exists, Snakemake assumes that step is complete. Solution: To re-run parts of the workflow, remove the flag files: rm flags/*.flag rm flags/*.done","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#q1-snakemake-reports-missing-output-files","text":"This usually happens because the output directory is on EOS or AFS , which may have some latency in reflecting newly created files. Solutions: Add the --latency-wait option when running Snakemake. Example: snakemake -j1 --latency-wait 600 This tells Snakemake to wait up to 600 seconds for missing files to appear. Make sure all Condor jobs have finished before running Snakemake again. You can check with: condor_q","title":"Q1: Snakemake reports missing output files?"},{"location":"faq/#q2-how-to-safely-cancel-running-condor-jobs","text":"Identify the jobs with: condor_q Cancel a specific job or job cluster with: condor_rm <job_id>","title":"Q2: How to safely cancel running Condor jobs?"},{"location":"faq/#q3-what-if-my-logs-dont-appear","text":"Ensure the log directory exists and is writable. If you run Condor jobs, logs may only appear after the job finishes on the batch system. Check the wrapper script path and permissions.","title":"Q3: What if my logs don\u2019t appear?"},{"location":"faq/#q4-why-does-snakemake-skip-the-ntuple-submission-step","text":"If the ntuple directory already exists in EOS: - Submission is skipped - A message is printed - he workflow continues safely This prevents accidental double submission to HTCondor.","title":"Q4: Why does Snakemake skip the ntuple submission step?"},{"location":"faq/#q5-snakemake-says-nothing-to-be-done-but-i-expected-more-steps-to-run","text":"This workflow uses flag files in the flags/ directory to track progress. If a flag exists, Snakemake assumes that step is complete. Solution: To re-run parts of the workflow, remove the flag files: rm flags/*.flag rm flags/*.done","title":"Q5: Snakemake says \u201cNothing to be done\u201d but I expected more steps to run"},{"location":"installation/","text":"Installation In order to set up the SM4Tops workflow, the following steps are needed: 1. Prerequisites This workflow assumes that all required external software environments are already installed and configured . The workflow does not attempt to create or validate these environments automatically . In particular, the following setups are required: FastFrames (ATLAS / StatAnalysis) Used for ntuple production via HTCondor. Requires a working ATLAS environment and a valid FastFrames installation. SALT + dumper-to-SALT Used to convert FastFrames ntuples into HDF5 files for MVA training. Please refer to the official SALT documentation for detailed setup instructions: Official SALT documentation Umami preprocessing Used for preprocessing and fold splitting prior to training. The workflow assumes that: setup.sh scripts for FastFrames, SALT, and Umami work when sourced manually SALT environments and packages are installed Required CVMFS and ATLAS tools are available EOS and AFS paths are accessible HTCondor is available for batch submission If any of these environments are missing or misconfigured, the workflow will fail at runtime. 2. Clone the workflow repository The workflow is hosted on GitHub: git clone https://github.com/Cecilia01atlas/cmse890-finale-project.git 3. Clone the required repositories This workflow relies on the following four git repositories provided by the ATLAS ftag group, which are hosted on GitLab: - fastframes - salt - dumper-to-salt - umami-preprocessing Access requires CERN credentials! git clone https://gitlab.cern.ch/atlasphys-top/TopPlusX/ANA-TOPQ-2023-43/sm4tops-fastframe.git git clone ttps://gitlab.cern.ch/atlas-phys/exot/hqt/ana-exot-2022-44/ftag-based-mva/salt.git git clone https://gitlab.cern.ch/atlas-phys/exot/hqt/ana-exot-2022-44/ftag-based-mva/dumper-to-salt.git git clone https://gitlab.cern.ch/atlas-phys/exot/hqt/ana-exot-2022-44/ftag-based-mva/umami-preprocessing.git Note: make sure to clone all repositories into a directory that matches the path specified in the workflow configuration (workflow_config.yaml) 4. Configure the workflow The workflow is configured via workflow_config.yaml . It uses a small set of base paths (AFS, EOS, project name, analysis tag) from which all tool-specific paths are derived automatically. Base configuration You must define: afs_root \u2013 Base AFS path for your CERN user eos_root \u2013 Base EOS path for large outputs project_name \u2013 Name of the project directory analysis_tag \u2013 Identifier for the analysis version FastFrames (ntuple production) Controls submission of FastFrames jobs to HTCondor: fastframe_repo \u2013 Path to the FastFrames repository fastframe_config \u2013 FastFrames YAML configuration custom_class \u2013 Custom FastFrames classes ntuples_eos_dir \u2013 EOS directory where ntuples are written Dumper-to-SALT (HDF5 conversion) Converts ntuples into .h5 files for ML training: samples \u2013 List of physics samples to process h5_output_dir \u2013 EOS directory for generated .h5 files Umami preprocessing Runs preprocessing over all folds: folds \u2013 List of folds (e.g. fold0 \u2013 fold3 ) preprocessing_config_dir \u2013 Directory with preprocessing YAML configs 5. Activate the snakemake environment conda activate <envrionment-name> 6. Test the installation You can run Snakemake in dry-run mode to check that everything is configured correctly: snakemake -n If paths or permissions are incorrect, Snakemake will report missing input files or directories.","title":"Installation"},{"location":"installation/#installation","text":"In order to set up the SM4Tops workflow, the following steps are needed:","title":"Installation"},{"location":"installation/#1-prerequisites","text":"This workflow assumes that all required external software environments are already installed and configured . The workflow does not attempt to create or validate these environments automatically . In particular, the following setups are required: FastFrames (ATLAS / StatAnalysis) Used for ntuple production via HTCondor. Requires a working ATLAS environment and a valid FastFrames installation. SALT + dumper-to-SALT Used to convert FastFrames ntuples into HDF5 files for MVA training. Please refer to the official SALT documentation for detailed setup instructions: Official SALT documentation Umami preprocessing Used for preprocessing and fold splitting prior to training. The workflow assumes that: setup.sh scripts for FastFrames, SALT, and Umami work when sourced manually SALT environments and packages are installed Required CVMFS and ATLAS tools are available EOS and AFS paths are accessible HTCondor is available for batch submission If any of these environments are missing or misconfigured, the workflow will fail at runtime.","title":"1. Prerequisites"},{"location":"installation/#2-clone-the-workflow-repository","text":"The workflow is hosted on GitHub: git clone https://github.com/Cecilia01atlas/cmse890-finale-project.git","title":"2. Clone the workflow repository"},{"location":"installation/#3-clone-the-required-repositories","text":"This workflow relies on the following four git repositories provided by the ATLAS ftag group, which are hosted on GitLab: - fastframes - salt - dumper-to-salt - umami-preprocessing Access requires CERN credentials! git clone https://gitlab.cern.ch/atlasphys-top/TopPlusX/ANA-TOPQ-2023-43/sm4tops-fastframe.git git clone ttps://gitlab.cern.ch/atlas-phys/exot/hqt/ana-exot-2022-44/ftag-based-mva/salt.git git clone https://gitlab.cern.ch/atlas-phys/exot/hqt/ana-exot-2022-44/ftag-based-mva/dumper-to-salt.git git clone https://gitlab.cern.ch/atlas-phys/exot/hqt/ana-exot-2022-44/ftag-based-mva/umami-preprocessing.git Note: make sure to clone all repositories into a directory that matches the path specified in the workflow configuration (workflow_config.yaml)","title":"3. Clone the required repositories"},{"location":"installation/#4-configure-the-workflow","text":"The workflow is configured via workflow_config.yaml . It uses a small set of base paths (AFS, EOS, project name, analysis tag) from which all tool-specific paths are derived automatically.","title":"4. Configure the workflow"},{"location":"installation/#base-configuration","text":"You must define: afs_root \u2013 Base AFS path for your CERN user eos_root \u2013 Base EOS path for large outputs project_name \u2013 Name of the project directory analysis_tag \u2013 Identifier for the analysis version","title":"Base configuration"},{"location":"installation/#fastframes-ntuple-production","text":"Controls submission of FastFrames jobs to HTCondor: fastframe_repo \u2013 Path to the FastFrames repository fastframe_config \u2013 FastFrames YAML configuration custom_class \u2013 Custom FastFrames classes ntuples_eos_dir \u2013 EOS directory where ntuples are written","title":"FastFrames (ntuple production)"},{"location":"installation/#dumper-to-salt-hdf5-conversion","text":"Converts ntuples into .h5 files for ML training: samples \u2013 List of physics samples to process h5_output_dir \u2013 EOS directory for generated .h5 files","title":"Dumper-to-SALT (HDF5 conversion)"},{"location":"installation/#umami-preprocessing","text":"Runs preprocessing over all folds: folds \u2013 List of folds (e.g. fold0 \u2013 fold3 ) preprocessing_config_dir \u2013 Directory with preprocessing YAML configs","title":"Umami preprocessing"},{"location":"installation/#5-activate-the-snakemake-environment","text":"conda activate <envrionment-name>","title":"5. Activate the snakemake environment"},{"location":"installation/#6-test-the-installation","text":"You can run Snakemake in dry-run mode to check that everything is configured correctly: snakemake -n If paths or permissions are incorrect, Snakemake will report missing input files or directories.","title":"6. Test the installation"},{"location":"workflow/","text":"Workflow Overview This project implements a multi-stage Snakemake workflow designed to prepare inputs for an MVA training using four-top Monte Carlo samples. This is being accomplished by using external tools like FastFrames, Salt and Umami. The entire workflow is structured as a Snakemake DAG : Workflow Stages 1. Ntuple Production (FastFrames) Reads a FastFrames configuration (e.g. Run3_config_DNN.yml ) Submits ntuple production jobs to HTCondor Stores ntuples in EOS Skips submission automatically if the ntuple directory already exists This step is asynchronous: once jobs are submitted, execution continues only after ntuples appear in EOS. 2. Ntuple Availability Check Periodically checks for the presence of the ntuple directory in EOS Handles EOS/AFS latency explicitly Acts as a synchronization point between batch production and local processing This allows the workflow to be stopped and resumed safely. 3. Conversion to HDF5 (dumper-to-SALT) Converts FastFrames ntuples into .h5 files Runs once per sample 4. Umami Preprocessing Runs preprocessing using Umami configuration files Processes all folds defined in the preprocessing configuration Produces preprocessed datasets ready for MVA training Workflow State Tracking (Flag Files) This workflow uses flag files (stored in the flags/ directory) to track progress between stages. Each major step creates a .flag or .done file once it has completed successfully. These files allow Snakemake to: Resume the workflow after interruptions Avoid re-running long or expensive steps Synchronize asynchronous batch jobs (e.g. HTCondor submissions) Important If you want to re-run parts of the workflow , the corresponding flag files must be removed manually. Helper Scripts Several workflow steps are executed via small wrapper scripts located in the scripts/ directory: run_fastframe.sh \u2014 submits FastFrames ntuple jobs run_dumper.sh \u2014 converts ntuples to HDF5 using dumper-to-SALT run_preprocessing.sh \u2014 runs Umami preprocessing These scripts must be executable. If you encounter permission errors, run: chmod +x scripts/*.sh Snakemake Rules Key rules in the workflow include: submit_ntuples Submits FastFrames jobs (or skips if outputs already exist) wait_for_ntuples Blocks execution until ntuples are available in EOS run_dumper Converts ntuples to HDF5 format per sample run_preprocessing Runs Umami preprocessing once all inputs are ready all Default entry point that executes the full workflow Example Snakemake Command To run the full workflow: snakemake -j1 --latency-wait 60 To execute only the initial stage (ntuple submission and availability check): snakemake stage1-j1 --latency-wait 60 Workflow Testing and Validation Snakemake provides built-in support for automatic unit test generation , which can be used to validate that individual rules behave as expected. Generating Unit Tests You can automatically generate unit tests for your workflow once you already run it using: snakemake --generate-unit-tests This command will: Create a tests/ directory (if it does not exist) Generate test cases for individual rules based on their inputs and outputs Capture expected file structure and execution logic","title":"Workflow Overview"},{"location":"workflow/#workflow-overview","text":"This project implements a multi-stage Snakemake workflow designed to prepare inputs for an MVA training using four-top Monte Carlo samples. This is being accomplished by using external tools like FastFrames, Salt and Umami. The entire workflow is structured as a Snakemake DAG :","title":"Workflow Overview"},{"location":"workflow/#workflow-stages","text":"","title":"Workflow Stages"},{"location":"workflow/#1-ntuple-production-fastframes","text":"Reads a FastFrames configuration (e.g. Run3_config_DNN.yml ) Submits ntuple production jobs to HTCondor Stores ntuples in EOS Skips submission automatically if the ntuple directory already exists This step is asynchronous: once jobs are submitted, execution continues only after ntuples appear in EOS.","title":"1. Ntuple Production (FastFrames)"},{"location":"workflow/#2-ntuple-availability-check","text":"Periodically checks for the presence of the ntuple directory in EOS Handles EOS/AFS latency explicitly Acts as a synchronization point between batch production and local processing This allows the workflow to be stopped and resumed safely.","title":"2. Ntuple Availability Check"},{"location":"workflow/#3-conversion-to-hdf5-dumper-to-salt","text":"Converts FastFrames ntuples into .h5 files Runs once per sample","title":"3. Conversion to HDF5 (dumper-to-SALT)"},{"location":"workflow/#4-umami-preprocessing","text":"Runs preprocessing using Umami configuration files Processes all folds defined in the preprocessing configuration Produces preprocessed datasets ready for MVA training","title":"4. Umami Preprocessing"},{"location":"workflow/#workflow-state-tracking-flag-files","text":"This workflow uses flag files (stored in the flags/ directory) to track progress between stages. Each major step creates a .flag or .done file once it has completed successfully. These files allow Snakemake to: Resume the workflow after interruptions Avoid re-running long or expensive steps Synchronize asynchronous batch jobs (e.g. HTCondor submissions)","title":"Workflow State Tracking (Flag Files)"},{"location":"workflow/#important","text":"If you want to re-run parts of the workflow , the corresponding flag files must be removed manually.","title":"Important"},{"location":"workflow/#helper-scripts","text":"Several workflow steps are executed via small wrapper scripts located in the scripts/ directory: run_fastframe.sh \u2014 submits FastFrames ntuple jobs run_dumper.sh \u2014 converts ntuples to HDF5 using dumper-to-SALT run_preprocessing.sh \u2014 runs Umami preprocessing These scripts must be executable. If you encounter permission errors, run: chmod +x scripts/*.sh","title":"Helper Scripts"},{"location":"workflow/#snakemake-rules","text":"Key rules in the workflow include: submit_ntuples Submits FastFrames jobs (or skips if outputs already exist) wait_for_ntuples Blocks execution until ntuples are available in EOS run_dumper Converts ntuples to HDF5 format per sample run_preprocessing Runs Umami preprocessing once all inputs are ready all Default entry point that executes the full workflow","title":"Snakemake Rules"},{"location":"workflow/#example-snakemake-command","text":"To run the full workflow: snakemake -j1 --latency-wait 60 To execute only the initial stage (ntuple submission and availability check): snakemake stage1-j1 --latency-wait 60","title":"Example Snakemake Command"},{"location":"workflow/#workflow-testing-and-validation","text":"Snakemake provides built-in support for automatic unit test generation , which can be used to validate that individual rules behave as expected.","title":"Workflow Testing and Validation"},{"location":"workflow/#generating-unit-tests","text":"You can automatically generate unit tests for your workflow once you already run it using: snakemake --generate-unit-tests This command will: Create a tests/ directory (if it does not exist) Generate test cases for individual rules based on their inputs and outputs Capture expected file structure and execution logic","title":"Generating Unit Tests"}]}